{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook will collect the necessary data \n",
    "# warning: ensure you put in a nice delay between queries as a courtesy to the server \n",
    "# as well as to not get kick off \n",
    "\n",
    "# author: marieke.van.erp@dh.huc.knaw.nl \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3958ecd",
   "metadata": {},
   "source": [
    "To gather the information for the mapping, perform the following 3 steps:  \n",
    "\n",
    "1. Run the following sparql queries on https://qlever.cs.uni-freiburg.de/wikidata\n",
    "\n",
    "PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?entity ?entityLabel ?property ?object \n",
    "WHERE\n",
    "  {\n",
    "  VALUES ?type { wd:Q43229 } . \n",
    "  ?entity wdt:P31*/wdt:P279* ?type .\n",
    "  ?entity ?property ?object . \n",
    "  ?entity rdfs:label ?entityLabel .\n",
    "  FILTER (LANG(?entityLabel) = \"en\") .\n",
    "}\n",
    "\n",
    "2. Download the results as tsv \n",
    "\n",
    "3. In a terminal, run the following command on the file to obtain a list of unique properties for the mapping: \n",
    "cut -f3 < <FILENAME> | sort | uniq > <UNIQUE_PROPERTIES> \n",
    "\n",
    "The properties were analysed manually, the resulting mapping file can be found in data/ and is further described in Section 5 of the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a441034",
   "metadata": {},
   "source": [
    "To obtain the data for the company, enterprise and business analyses (Section 6 in the paper): \n",
    "1. Run the following sparql queries on https://query.wikidata.org/ to get the business entities: \n",
    "\n",
    "SELECT ?entity ?entityLabel\n",
    "WHERE\n",
    "{ \n",
    "  ?entity wdt:P31 wd:Q4830453 . \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } \n",
    "}\n",
    "\n",
    "2. Save the results as businesses.tsv without headers \n",
    "\n",
    "3. Run the following sparql queries on https://query.wikidata.org/ to get the enterprise entities: \n",
    "SELECT ?entity ?entityLabel\n",
    "WHERE\n",
    "{ \n",
    "  ?entity wdt:P31 wd:Q6881511 . \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } \n",
    "}\n",
    "\n",
    "4. Save the results as enterprises.tsv without headers \n",
    "\n",
    "5. Run the following sparql queries on https://query.wikidata.org/ to get the company entities: \n",
    "SELECT ?entity ?entityLabel\n",
    "WHERE\n",
    "{ \n",
    "  ?entity wdt:P31 wd:Q783794 . \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". } \n",
    "}\n",
    "\n",
    "6. Save the results as companies.tsv without headers \n",
    "\n",
    "Note: a combined query such as the following should return the same results, but probably results in a timeout: \n",
    "   SELECT ?entity ?entityLabel\n",
    "   WHERE\n",
    "    { \n",
    "    VALUES ?type { wd:Q4830453 wd:Q6881511 wd:Q783794 } .  \n",
    "    ?entity wdt:P31 ?type . \n",
    "    } \n",
    "    \n",
    "7. Combine the three files to get a list of unique ids and labels. For example by the following command: \n",
    "cat businesses.tsv enterprises.tsv companies.tsv >> businesses-enterprises-companies.tsv \n",
    "\n",
    "8. Ensure the file is in the directory data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61451a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble: load the necessary modules \n",
    "import pandas as pd\n",
    "import pywikibot\n",
    "import glob \n",
    "import time\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble: tell the pywikibot which database to query: \n",
    "site = pywikibot.Site('wikidata', 'wikidata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabaaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble: function that reads in an entity id, queries Wikidata for a set of properties about that entity \n",
    "# and how to store the information \n",
    "def getchangeinfo(site, item_qid, claims_of_interest):\n",
    "    item = pywikibot.ItemPage(site, item_qid)\n",
    "    info = []\n",
    "    \n",
    "    try:\n",
    "        item_dict = item.get() #Get the item dictionary\n",
    "    \n",
    "        item_name = \"DUMMY-no-en-label-DUMMY\"\n",
    "    \n",
    "        if \"en\" in item_dict[\"labels\"]:\n",
    "            item_name = item_dict[\"labels\"][\"en\"]\n",
    "    \n",
    "        clm_dict = item_dict[\"claims\"] # Get the claim dictionary\n",
    "        \n",
    "        for coi in claims_of_interest:\n",
    "            #print(\"Processing: \" + str(coi))\n",
    "            claim_lookup = coi[0]\n",
    "            claim_lookup_name = coi[1]\n",
    "            start_time = None\n",
    "            end_time = None\n",
    "            point_in_time  = None\n",
    "            try:\n",
    "                claims = clm_dict[claim_lookup]\n",
    "                for c in claims:\n",
    "                    #print(\"yay\")\n",
    "                    clm_trgt = c.getTarget()\n",
    "                    targetid = None\n",
    "                    targetstr = str(clm_trgt)\n",
    "                    if type(clm_trgt) is pywikibot.WbMonolingualText:\n",
    "                        targetstr = clm_trgt.text\n",
    "                    if type(clm_trgt) is pywikibot.ItemPage:\n",
    "                        targetstr = clm_trgt.labels['en']\n",
    "                        targetid = clm_trgt.title()\n",
    "                    if type(clm_trgt) is pywikibot.WbQuantity:\n",
    "                        targetstr = clm_trgt.amount\n",
    "                    else:\n",
    "                        pass\n",
    "                    if c.qualifiers:\n",
    "                        for qualifier_property, qualifiers in c.qualifiers.items():\n",
    "                            for qualifier in qualifiers:\n",
    "                                ## end time\n",
    "                                if qualifier_property == \"P582\":\n",
    "                                    end_time = qualifier.getTarget().toTimestr()\n",
    "                                ## start time    \n",
    "                                if qualifier_property == \"P580\":\n",
    "                                    start_time = qualifier.getTarget().toTimestr()\n",
    "                                if qualifier_property == \"P585\":\n",
    "                                    point_in_time = qualifier.getTarget().toTimestr()\n",
    "\n",
    "                            #   info.append((item_qid, claim_lookup, targetid, targetstr, start_time, end_time, point_in_time))\n",
    "                    info.append((item_qid, item_name, claim_lookup, claim_lookup_name, targetid, targetstr, start_time, end_time, point_in_time))\n",
    "                    start_time = None\n",
    "                    end_time = None\n",
    "                    point_in_time  = None\n",
    "                    #print(info)\n",
    "                \n",
    "            except:\n",
    "                # claim_lookup == None\n",
    "                # claim_lookup_name == None\n",
    "                # targetid = None\n",
    "                #  targetstr = None\n",
    "                #  info.append((item_qid, item_name, claim_lookup, claim_lookup_name, targetid, targetstr, start_time, end_time, point_in_time))\n",
    "                #  print(\"nay\")\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if it works \n",
    "r = getchangeinfo(site, 'Q25936435', [(\"P1546\", \"motto\"), (\"P1451\", \"motto text\")])\n",
    "\n",
    "for i in r:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main part: This is where you read in the file with the ids and names to get other information \n",
    "# about the entities \n",
    "# If you want, you can also split up the file into smaller files to run parallel queries or keep track more easily,\n",
    "# hence, the glob option\n",
    "# Grab a cup of tea, this will take a while \n",
    "files = glob.glob(\"data/businesses-enterprises-companies.tsv\")\n",
    "df1 = pd.DataFrame(columns=['item_qid', 'item_name', 'claim_lookup', 'claim_lookup_name', 'targetid', 'targetstr', 'start_time', 'end_time', 'point_in_time'])\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    time.sleep(0.7) # be nice \n",
    "    start = datetime.datetime.now()\n",
    "    out_tsv = file + '_info.tsv'\n",
    "    print(file, out_tsv)\n",
    "    df1.to_csv(out_tsv, index=False,header=True,mode='a', sep='\\t')\n",
    "    input_list = pd.read_csv(file, sep='\\t', names=[\"company\"]).drop_duplicates()\n",
    "   \n",
    "\n",
    "    ## Things to query:\n",
    "    # P1546 motto \n",
    "    # P1451 motto_text \n",
    "    # P1448 name \n",
    "    # P452 industry \n",
    "    # P1830 owner_of \n",
    "    # P355 subsidiary \n",
    "    # P169 ceo \n",
    "    # P1128 number_of_employees \n",
    "    # P1813 acronym \n",
    "\n",
    "    for index, row in input_list.iterrows():\n",
    "        # print some info to know where we're at \n",
    "        print(row['company'])\n",
    "        if '\"' in row['company']:\n",
    "            continue\n",
    "        r = getchangeinfo(site, row['company'],[(\"P1546\", \"motto\"), (\"P1451\", \"motto text\"), \n",
    "                                           (\"P1448\",\"name\"), (\"P452\",\"industry\"), (\"P1830\",\"owner of\"),\n",
    "                                           (\"P355\",\"subsidiary\"), (\"P169\",\"ceo\"), (\"P1128\",\"number of employees\"),\n",
    "                                           (\"P1813\",\"acronym\")])\n",
    "        df2 = pd.DataFrame(list(r), columns=['item_qid', 'item_name', 'claim_lookup', 'claim_lookup_name', 'targetid', 'targetstr', 'start_time', 'end_time', 'point_in_time'])\n",
    "        df2.to_csv(out_tsv, index=False,header=False,mode='a', sep='\\t')\n",
    "    \n",
    "    end = datetime.datetime.now() \n",
    "    print(\"start: \" +  str(start) +  \" end: \"+ str(end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630a639",
   "metadata": {},
   "source": [
    "We need a bit more information, namely the industry of the subsidiaries \n",
    "\n",
    "For this, you need to load in the file you just created, pull out the subsidiary ids and query those again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file into a dataframe \n",
    "files = glob.glob('data/businesses-enterprises-companies.tsv_info.tsv') #yes the name is somewhat ugly \n",
    "\n",
    "df = []\n",
    "li = []\n",
    "\n",
    "for filename in files:\n",
    "    print(filename)\n",
    "    temp_frame = pd.read_csv(filename, index_col=None, header=0, sep=\"\\t\")\n",
    "    li.append(temp_frame)\n",
    "    \n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the subsidiary rows \n",
    "sub_data = df.loc[df['claim_lookup'].isin(['P1830', 'P355'])].drop_duplicates() \n",
    "sub_data['occurrences'] = sub_data.groupby('item_qid')['item_qid'].transform('size')\n",
    "sub_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a tsv \n",
    "sub_data.to_csv('subsidiaries_company_enterprise_business.tsv', index=False,header=False,mode='a', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data for the subsidiaries' industries \n",
    "# Grab a cup of tea, this will take a while \n",
    "files = glob.glob(\"subsidiaries_company_enterprise_business.tsv\")\n",
    "df1 = pd.DataFrame(columns=['item_qid', 'item_name', 'claim_lookup', 'claim_lookup_name', 'targetid', 'targetstr', 'start_time', 'end_time', 'point_in_time'])\n",
    "x = 1 \n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    time.sleep(0.1)\n",
    "    start = datetime.datetime.now()\n",
    "    out_tsv = file + '_final_info.tsv'\n",
    "    print(file, out_tsv)\n",
    "    df1.to_csv(out_tsv, index=False,header=True,mode='a', sep='\\t')\n",
    "    #input_list = pd.read_csv(file, sep='\\t', names=[\"company\",\"companyLabel\"]).drop_duplicates()\n",
    "    input_list = pd.read_csv(file, sep='\\t', names=[\"company\"]).drop_duplicates()\n",
    "    #input_list['company'] = input_list['company'].str.replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "\n",
    "    for index, row in input_list.iterrows():\n",
    "        print(x, row['company'])\n",
    "        x = x+1 \n",
    "        r = getchangeinfo(site, row['company'],[(\"P452\",\"industry\"), (\"P1448\", \"name\")])\n",
    "        df2 = pd.DataFrame(list(r), columns=['item_qid', 'item_name', 'claim_lookup', 'claim_lookup_name', 'targetid', 'targetstr', 'start_time', 'end_time', 'point_in_time'])\n",
    "        df2.to_csv(out_tsv, index=False,header=False,mode='a', sep='\\t')\n",
    "    \n",
    "    end = datetime.datetime.now() \n",
    "    print(\"start: \" +  str(start) +  \" end: \"+ str(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f9fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
